# NOTES



> Purpose of report:
>
> ​	xfolds:
>
> * give effective making techniques for longer sequence lengths (compare)
> * scaling law to give appropriately sized parameter model
> * to multiple adapters training or to single encoder decoder training (but this with longer sequence lengths)
> *  compare results with previous similarly sized models.



MedClip https://huggingface.co/flaviagiammarino/pubmed-clip-vit-base-patch32

PubMedCLIP was trained on the [Radiology Objects in COntext (ROCO)](https://github.com/razorx89/roco-dataset) dataset, a large-scale multimodal medical imaging dataset. The ROCO dataset includes diverse imaging modalities (such as X-Ray,  MRI, ultrasound, fluoroscopy, etc.) from various human body regions  (such as head, spine, chest, abdomen, etc.)  captured from open-access [PubMed](https://pubmed.ncbi.nlm.nih.gov/) articles.



PubMedCLIP was trained for 50 epochs with a batch size of 64 using the Adam optimizer with a learning rate of 10−5.  The authors have released three different pre-trained models at this [link](https://1drv.ms/u/s!ApXgPqe9kykTgwD4Np3-f7ODAot8?e=zLVlJ2)  which use ResNet-50, ResNet-50x4 and ViT32 as image encoders. This  repository includes only the ViT32 variant of the PubMedCLIP model.



- **Repository:** [PubMedCLIP Official GitHub Repository](https://github.com/sarahESL/PubMedCLIP)
- **Paper:** [Does CLIP Benefit Visual Question Answering in the Medical Domain as Much as it Does in the General Domain?](https://arxiv.org/abs/2112.13906)
- Current approaches for this multimodal task adopt deep neural encoders to interpret the image and the question and then pick a corresponding answer. They typically consist of four main components: a visual encoder, question encoder, attention-based fusion of vision and text features, and an answer classiﬁer Vu et al. [2020], Zhan et al. [2020], Nguyen et al. [2019], Pan et al. [2021], Liu et al. [2021b].

> [Ideas]
>
> They already used 2 different Encoders for text and images[specialized].
>
> so now we can just try to make 1 unified encoder and do ablation
>
> [we arent doing ultrasound] so would it be fair comparison. so maybe just use their tokenizer.?

## other notes

* find benchmark datasets if you dont use ultraSound , flouroscopy