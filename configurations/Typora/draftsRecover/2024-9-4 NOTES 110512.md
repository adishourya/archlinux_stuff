# NOTES



MedClip https://huggingface.co/flaviagiammarino/pubmed-clip-vit-base-patch32

PubMedCLIP was trained on the [Radiology Objects in COntext (ROCO)](https://github.com/razorx89/roco-dataset) dataset, a large-scale multimodal medical imaging dataset. The ROCO dataset includes diverse imaging modalities (such as X-Ray,  MRI, ultrasound, fluoroscopy, etc.) from various human body regions  (such as head, spine, chest, abdomen, etc.)  captured from open-access [PubMed](https://pubmed.ncbi.nlm.nih.gov/) articles.



PubMedCLIP was trained for 50 epochs with a batch size of 64 using the Adam optimizer with a learning rate of 10âˆ’5.  The authors have released three different pre-trained models at this [link](https://1drv.ms/u/s!ApXgPqe9kykTgwD4Np3-f7ODAot8?e=zLVlJ2)  which use ResNet-50, ResNet-50x4 and ViT32 as image encoders. This  repository includes only the ViT32 variant of the PubMedCLIP model.



- **Repository:** [PubMedCLIP Official GitHub Repository](https://github.com/sarahESL/PubMedCLIP)
- **Paper:** [Does CLIP Benefit Visual Question Answering in the Medical Domain as Much as it Does in the General Domain?](https://arxiv.org/abs/2112.13906)

> [Ideas]
>
> They already used 2 different Encoders for text and images[specialized].
>
> so now we can just try to make 1 unified encoder and do ablation
>
> [we arent doing ultrasound] so would it be fair comparison. so maybe just use their tokenizer.?



## other notes

* find benchmark datasets if you dont use ultraSound , flouroscopy