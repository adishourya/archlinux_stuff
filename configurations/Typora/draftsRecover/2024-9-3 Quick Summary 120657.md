# Quick Summer Summary ðŸ˜…

I read an article from Korean Journal of Radiology which had some nice things to say about multimodal models on radiology and health care.

The issues it mostly talks about were : 

* achieving generalizability of AI models

* establishing the explainability of the decision-making process

  

-> https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10613849/

>To overcome these issues, some of the various possible solutions are as follows:
>
>inclusion of training with longitudinal and multimodal datasets, dense training with multitask and multimodal learning, new  generative models including anomaly detection, XAI
>
>--- From Summary and conclusion

---



I Mostly read on Collecting data and tokenization , since that will be our first steps.



## For Data:

* Medpix2.0

  * 12,000 patient case scenarios,9,000 topics, and nearly 59,000 images.

  * But Far less than what 4m pretrains on which is 12M (CC12M).

  * But Medpix claims : `CLIP succeeds in achieving competitive performance in zero-shotcontexts on a wide range of classification datasets by learning the relationshipsbetween images and their textual descriptions ` entirely trained on their own dataset.

    


## For tokenization

* Here 4m does different than almost every other paper
* 4m has 1 unified embedding for all modalities
* But Llama , Medpix and others have different image and text encoder. [so as many embeddings and cross attention as modalities]

## Extra Ideas

Just so that its not complete replication of any 1 paper. I was thinking of making some changes by taking pieces of others architectures so that i could do ablation studies [Thats a research gap i think].

* like 4m does not use grouped query attention (unlike Llama 3.1).
* 4m does a lot of masking (Massively Masked Multiple Modality) 

